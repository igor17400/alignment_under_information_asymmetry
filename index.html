<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Alignment Under Information Asymmetry</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <div class="container">
        <header>
            <h1>Alignment Under Information Asymmetry</h1>
            <p class="subtitle">Research Project</p>
        </header>

        <section>
            <h2>About</h2>
            <p>
                This research project investigates the challenges and solutions for AI alignment
                in scenarios where there exists information asymmetry between different agents or
                between AI systems and human stakeholders. Understanding how to maintain alignment
                under such conditions is crucial for the development of safe and beneficial AI systems.
            </p>
            <p>
                The project explores theoretical frameworks and practical approaches to ensure that
                AI systems remain aligned with human values and intentions even when operating with
                incomplete or asymmetric information.
            </p>
        </section>

        <section>
            <h2>Repository</h2>
            <div class="links">
                <a href="https://github.com/igorlimarochaazevedo/alignment_under_information_asymmetry" target="_blank">View on GitHub</a>
            </div>
        </section>

        <section>
            <h2>Contact</h2>
            <p>
                For more information about this research project, please reach out via the GitHub repository
                or through the contact information provided in the project documentation.
            </p>
        </section>

        <footer>
            <p>&copy; 2025 Alignment Under Information Asymmetry Research Project</p>
        </footer>
    </div>
</body>
</html>
